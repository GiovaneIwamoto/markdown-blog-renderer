---
title: "Module 3: Retrieval-Augmented Generation"
layout: post
---

In this module, we'll explore how language models can be enhanced by connecting them to external sources of information—a technique known as Retrieval-Augmented Generation (RAG). While LLMs are powerful, they're limited to the data they were trained on. RAG allows the model to retrieve external content in real time and use it as context for generating more accurate and relevant responses.


<div style="display: block; text-align: center;">

  <img src="{{ '/assets/images/langchain_indexing.png' | relative_url }}" alt="Langchain Indexing" 
       style="width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto 10px auto;" />

  <img src="{{ '/assets/images/langchain_retrieval.png' | relative_url }}" alt="Langchain Retrieval" 
       style="width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto;" />

</div>

*Image source: [LangChain Documentation](https://python.langchain.com/docs/tutorials/rag/)*

To build a RAG system, we'll cover the full pipeline: loading data from various sources, splitting long texts into chunks, embedding the content into vectors, storing it in vector databases, retrieving the most relevant pieces during a query, and finally using that retrieved information to generate informed answers.

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/T-D1OfcDW1M" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/zYGDpG-pTho" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

</div>

Before we begin exploring each topic individually, here's a brief overview of the core components involved in the Retrieval-Augmented Generation (RAG) workflow. Each step plays a crucial role in enabling language models to retrieve and use external information effectively:

* **Document Loaders**: Responsible for bringing external data into the system. They support a wide variety of formats such as PDFs, web pages, databases, and cloud files, acting as the entry point to your knowledge base.

* **Text Splitters**: Once documents are loaded, they are broken down into smaller, more manageable chunks. This helps ensure that information remains coherent and accessible during processing and retrieval.

* **Text Embedding Models**: These models convert chunks of text into numerical vectors that capture the meaning and context of the content. This transformation enables semantic search, where similar concepts are positioned closely in vector space.

* **Vector Stores**: Specialized databases that store and organize text embeddings. They allow for fast and meaningful retrieval of content and are compatible with in-memory setups or scalable cloud-based solutions.

* **Retrievers**: Tools that search the vector store to find the most relevant pieces of information based on a user query. Different types of retrievers exist, from simple semantic matchers to more advanced ones like self-query retrievers or ensemble approaches that combine multiple strategies.

* **Generation**: Finally, the language model takes the retrieved context and uses it to generate responses that are more accurate, grounded, and relevant to the query.

*The video below offers a high-level overview of the key topics we’ll be working on in this module. It serves as a helpful preview of what’s to come, and each component will be explored individually and in more depth as we move forward.*

{% include embed.html url="https://www.youtube.com/embed/tcqEUSNCn8I" %}

---

# **1. Document Loaders**

Document Loaders are responsible for bringing external content—like CSVs, PDFs, web pages, and databases—into your application in a format that LangChain can work with. Regardless of the source, all loaders return data in a unified structure called a `Document`, which includes both the content and relevant metadata.

Each type of loader is designed to handle a specific format or data source, and while their configuration parameters vary depending on the integration, they all share a common interface: you initialize the loader and call its `.load()` method to extract the data.

```python
# Load CSV file
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader( 
    ...  # Your custom parameters here
)
documents = loader.load()
```

```python
# Load PDF file
from langchain_community.document_loaders import PyPDFLoader

file_path = "./example_data/layout-parser-paper.pdf"
loader = PyPDFLoader(file_path)

documents = loader.load()
```

<div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/integrations/document_loaders/" target="_blank">
      <img src="{{ '/assets/images/lc_document_loaders.png' | relative_url }}" alt="Langchain Document Loaders"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/how_to/#document-loaders" target="_blank">
      <img src="{{ '/assets/images/lc_how_to.png' | relative_url }}" alt="Langchain How To Document Loaders"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>

  <div style="flex: 1;">
    <p>
      <em>
        To explore how document loaders work and understand the different types available, the links provided offer both a practical guide to using them effectively and a broader overview of their role in connecting AI workflows to real-world data.
      </em>
    </p>
  </div>

</div>

---

# **2. Text Splitters**

Text splitters are utilities that break a large document into smaller chunks of text. In a Retrieval-Augmented Generation (RAG) pipeline, this step is crucial: it transforms long knowledge sources into bite-sized pieces that can be embedded and retrieved efficiently. Large Language Models have a limited context window, meaning they cannot consume extremely long texts all at once.

By splitting a document into chunks that fit the context window, we ensure the LLM can later ingest relevant pieces. This process (often called chunking) is fundamental to RAG – after splitting, each chunk can be vectorized (embedded) and stored, so that given a query, only the most relevant chunks are retrieved for the model to generate an answer. 

In short, text splitters bridge the gap between dense source data and LLM input limits, enabling applications like question answering over documents.

{% include embed.html url="https://www.youtube.com/embed/Io43kf0hYn4" %}

Another reason text splitters are important is semantic coherence. Ideally, we want each chunk to contain a self-contained, semantically meaningful piece of information. 

If we naively split text at fixed lengths, we might cut off sentences or separate closely related context. Good text splitting keeps related pieces of text together – for example, a paragraph or a section of a document should remain in one chunk if possible. Maintaining this coherence improves the quality of downstream embeddings and results, since each chunk will represent a complete thought or topic.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://chunkviz.up.railway.app" target="_blank">
      <img src="{{ '/assets/images/chunk_viz.png' | relative_url }}" alt="Chunkviz"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        To better visualize how different chunking strategies work and understand their impact, check out this interactive tool: ChunkViz v0.1. It's a helpful resource for exploring various text splitting approaches in the context of language models.
      </em>
    </p>
  </div>

</div>

### How Text Splitters Work in LangChain

LangChain provides built-in text splitter classes that implement common chunking strategies. Under the hood, the chunking process can be summarized in three steps:

**Initial Split** – Break the text into small units (e.g. sentences or paragraphs) using a separator or delimiter. This yields many small pieces that are semantically meaningful (for example, splitting on sentence boundaries).

**Chunk Assembly** – Gradually combine those small pieces into a larger chunk until a certain size limit is reached (the chunk size). This forms a chunk that is under or around the desired maximum length.

**Overlap and Next Chunk** – Once a chunk reaches the size limit, finalize it and start a new chunk. Optionally include some overlap with the previous chunk (repeating the last few tokens/words) to preserve context continuity between chunks. This overlap helps to avoid cutting important information that straddles a boundary.

This chunking approach aims to produce text segments that are constrained in size while still preserving the natural structure of the original content. Instead of arbitrarily cutting text at fixed lengths—which can disrupt meaning or break sentences—this method prioritizes splitting at logical boundaries such as paragraphs or sentences. 

There are two important aspects you can configure to fine-tune how chunking behaves. First, how the text is split: this could be based on characters, words, sentences, or specific delimiters like newlines or paragraph breaks. Choosing the right splitting method helps maintain the coherence of each chunk. Second, how the chunk size is measured: chunk limits can be defined by the number of characters, the number of tokens (which is especially relevant for language models), or even by higher-level semantic units. 

By thoughtfully adjusting these two factors, you can ensure that each chunk remains meaningful, balanced in size, and well-suited for downstream tasks like embedding or retrieval.

<div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
  <div style="flex: 0 0 20%;">
    <a href="https://python.langchain.com/docs/how_to/#text-splitters" target="_blank">
      <img src="{{ '/assets/images/lc_how_to.png' | relative_url }}" alt="Langchain How To Text Splitters"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 0 0 20%;">
    <a href="https://python.langchain.com/docs/concepts/text_splitters/" target="_blank">
      <img src="{{ '/assets/images/lc_text_splitters.png' | relative_url }}" alt="Langchain Text Splitters"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>

  <div style="flex: 1;">
    <p>
      <em>
        For more information on the different types of text splitters and how to choose the right one for your use case, check out the official documentation linked in the images.
      </em>
    </p>
  </div>

</div>

---

# **3. Embedding Models**

{% include embed.html url="https://www.youtube.com/embed/vlcQV4j2kTo" %}

Text embeddings are numerical representations of text that capture its semantic meaning and context in a high-dimensional space. They enable machines to assess the similarity between texts based on meaning, rather than relying on exact word matching.

This process begins with text vectorization, which converts text into a machine-readable numeric format. Traditional methods like one-hot encoding or bag-of-words are limited, as they fail to capture contextual meaning. Modern embedding models overcome this by mapping similar texts to nearby points in a shared vector space, enabling more meaningful comparisons.

In Natural Language Processing (NLP), embeddings support a wide range of applications such as semantic search, document clustering, content recommendations, anomaly detection, and classification. By using metrics like cosine similarity, embeddings allow for quantitative analysis of semantic relationships between text inputs.

OpenAI provides high-quality embedding models capable of generating fixed-length vectors that capture the semantic essence of text. These embeddings typically have high dimensionality and are well-suited for tasks such as similarity search, clustering, and classification. Using LangChain, these models can be easily integrated into applications through intuitive interfaces and built-in support for vector storage and retrieval.

Ultimately, embeddings are a critical component in building intelligent applications that understand language contextually, supporting more accurate and human-like interactions.

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")

embeddings = embeddings_model.embed_documents(
    ["At Dandilyonn, we strongly believe in the statement Earth is Worth it: that the Earth is worth saving, and we will stop at nothing to ensure that our community is a clean energy, green machine. Our mission is to educate youth about climate change, promote green habits, and raise money for community projects beneficial to the environment.",]
)
len(embeddings), len(embeddings[0])

# Output
(1, 3072)

embedded_query = embeddings_model.embed_query("What is SEEDS and what is its main purpose?")
embedded_query[:6]

# Output
[-0.02120787650346756,
 0.0007380267488770187,
 -0.02360207587480545,
 -0.036326803267002106,
 0.00472189299762249,
 -0.032632049173116684]
```
{% include embed.html url="https://www.youtube.com/embed/wgfSDrqYMJ4" %}

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/concepts/embedding_models/" target="_blank">
      <img src="{{ '/assets/images/lc_embedding_models.png' | relative_url }}" alt="Langchain Embedding Models"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation. This is the power of embedding models, which lie at the heart of many retrieval systems. Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.
      </em>
    </p>
  </div>

</div>