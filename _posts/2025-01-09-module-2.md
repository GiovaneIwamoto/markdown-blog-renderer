---
title: "Module 2: Output Parsers, Chains, Context Window, Chat History, Trimmer, Chatbot, Memory"
layout: post
---

Explore the key components that make LangChain capable of building dynamic workflows: chains that connect model calls, output parsers that clean and structure responses, context windows and chat history that keep conversations coherent, trimmers that control history length, and chatbots that use memory to handle multi-turn interactions. You will also get introduced to LangGraph, which helps manage conversation state and persistence. By the end, you'll understand how these elements work together to create richer applications and you’ll be ready for the next step, where we'll dive deeper into advanced LangGraph features and agents.


*The video below is a recap of what we covered in the previous module and also gives a brief introduction to the concepts of parsers and chains, which we will explore in more depth throughout this module.*

{% include embed.html url="https://www.youtube.com/embed/mrjq3lFz23s" %}

---

# **1. Output Parsers**

Output parsers in LangChain are essential tools that transform raw, unstructured responses from language models into clean, structured formats like strings, lists, dictionaries, or validated objects. This is important because raw text can often be unpredictable, inconsistent, or hard to process directly in code. By using output parsers, you ensure that outputs follow a predictable structure, making them easier to integrate into applications, trigger follow-up actions, or pass to downstream systems.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/concepts/output_parsers/" target="_blank">
      <img src="{{ '/assets/images/langchain_output_parsers.png' | relative_url }}" alt="Langchain Output Parsers"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Check the official documentation on output parsers and the different types of output parsers available, so you can understand their purpose and choose the right one for your specific use case.
      </em>
    </p>
  </div>

</div>

LangChain offers a variety of built-in output parsers, such as `StrOutputParser` for plain text, `CommaSeparatedListOutputParser` for lists, and `PydanticOutputParser` for structured data that follows defined schemas. These tools help reduce the risk of errors, improve automation reliability, and enforce data quality standards, especially when chaining multiple steps together or when interacting with external tools and APIs that require well-formed, consistent input.

*Check the video below to better understand what output parsers are and how they help improve the results you get from your models. The video covers key types such as the Structured Output Parser, CommaSeparatedList Output Parser, Pydantic Output Parser, Output Fixing Parser, and Retry Output Parser, with practical examples of when and how to use each.*

{% include embed.html url="https://www.youtube.com/embed/UVn2NroKQCw" %}

---

# **2. Chains**

In LangChain, a Chain is a structured workflow where data moves through a series of components, with each component performing a specific task. Instead of relying on one big model call, chains break the process into smaller, modular steps — for example, formatting a prompt, calling a language model, and parsing the output.

This modular design makes it easier to:

- Connect multiple steps into a meaningful sequence
- Reuse common workflows across different applications
- Hide underlying complexity, so you can focus on the bigger logic

In short, a chain is like a pipeline that transforms inputs into outputs by passing them through well-defined stages.

### LCEL (LangChain Expression Language)

LCEL is the modern approach to building chains in LangChain. It provides a more flexible and composable way to create chains using a declarative syntax for composing **Runnables** into chains using the pipe operator `|`. This approach simplifies the construction of complex workflows by allowing you to define the sequence of operations in a readable and concise manner.

> But wait — before we continue: **what exactly is a Runnable?**

It represents any unit of work that can process inputs and produce outputs — for example, a language model, a prompt template, an output parser, a retriever, or even an entire chain.

What makes Runnables powerful is that they all share a **standardized interface**. No matter what specific task they perform, they provide the same key methods:

`.invoke() `→ processes a single input synchronously and returns the output.

`.batch()` → processes a list of inputs in parallel and returns a list of outputs.

`.stream()` → processes an input and streams the output incrementally.

`.astream_log()` → streams both the final output and selected intermediate steps.

With this foundation in mind, we can now understand why LCEL is so effective: by treating all components as Runnables, LCEL allows you to seamlessly compose them into powerful, multi-step workflows using the simple pipe syntax. Whether you're connecting models, parsers, retrievers, or custom functions, LCEL provides a unified, elegant way to chain these pieces together — all while automatically supporting synchronous, asynchronous, batch, and streaming execution.

*Now let's look at an example that puts into practice the concepts of LCEL, chains, and output parsers, showing how they work together to create a seamless and structured workflow.*

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Define a prompt template
prompt = ChatPromptTemplate.from_template(
    "Write a 5-line poem about {topic}."
)

# Initialize a model
model = ChatOpenAI(model="gpt-3.5-turbo")

# Use an output parser to get a string
output_parser = StrOutputParser()

# Build the chain with LCEL
chain = prompt | model | output_parser

# Run the chain with .invoke()
result = chain.invoke({"topic": "AI Agents, Education and Seeds"})
```

*Output:*

```s
result = """AI agents sow seeds of knowledge,
In the fertile fields of education,
Nurturing growth and understanding,
With algorithms and innovation,
Harvesting a future of endless possibilities."""
```

*Below are two articles that provide a deeper explanation of chains and output parsers, respectively. I recommend reviewing them to strengthen your understanding of how these components work.*

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">
  
  <a href="https://medium.com/@shravankoninti/different-chain-types-using-langchain-89cacae6ad1f" target="_blank" style="flex: 1 1 28%; box-sizing: border-box; overflow: hidden;">
    <img src="{{ '/assets/images/langchain_chain_types.png' | relative_url }}" alt="Langchain Chain Types"
         style="width: 100%; height: 93%; object-fit: cover; border-radius: 8px;" />
  </a>

  <a href="https://medium.com/donato-story/unpacking-output-parsers-with-langchain-d84ccaff8d3c" target="_blank" style="flex: 1 1 28%; box-sizing: border-box;">
    <img src="{{ '/assets/images/medium_parsers.png' | relative_url }}" alt="Medium Parsers"
         style="width: 100%; height: auto; object-fit: cover; border-radius: 8px;" />
  </a>

</div>

*Here are the official documentation references for the terms we've discussed. While reviewing them is optional, it's highly recommended since they are the most reliable source of information, offering deeper explanations, practical examples, and helping you develop a stronger, more accurate understanding of these concepts in action.*

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: wrap;">

  <a href="https://python.langchain.com/docs/concepts/lcel/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_lcel.png' | relative_url }}" alt="Langchain LCEL"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/how_to/sequence/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_chain.png' | relative_url }}" alt="Langchain How To Chain Runnables"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/how_to/parallel/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_parallel.png' | relative_url }}" alt="Langchain Runnables in Parallel"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/concepts/runnables/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_runnable.png' | relative_url }}" alt="Langchain Runnable Interface"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

</div>

---

# **3. Context Window**

Before we dive deeper into topics like memory, message history management, and how chatbots keep track of conversations, **we first need to understand the concept of the context window** in Large Language Models.

The context window is basically the amount of information — measured in tokens — that the model can “see” and process at once. This includes not just the most recent user message, but also past conversation turns, instructions, and the model's own generated responses.

Why is this important? Because LLMs don't have memory like humans. They can only work with what's inside this limited window. If your conversation or task is too long and goes beyond the window size, earlier parts get cut off or lost.

*Please check the video below to better understand the concept of a context window:*

{% include embed.html url="https://www.youtube.com/embed/-QVoIxEpFkM" %}

---

# **4. Chat History**

**Chat History** refers to the chronological record of interactions between a user and a chat model. It plays a crucial role in preserving the context and state of a conversation by storing a sequence of messages, where each message is tagged with a specific role — such as `user`, `assistant`, `system`, or `tool` — that indicates its source or purpose.

Up to this point, we've already learned about these roles and how they shape the flow of an interaction. But when we combine them into a running sequence — the chat history — we unlock one of the most important capabilities for building real conversational applications.

Why? Because in a conversational setting, the system cannot rely only on the most recent input. It needs to remember prior exchanges, decisions, clarifications, and even tool calls to respond meaningfully. Without chat history, each new turn would be treated in isolation, making it impossible for the assistant to handle follow-up questions, track topics, or maintain coherent, multi-turn interactions.

Let's break this down:

Most conversations begin with a system message that sets the initial instructions or tone. After that, the user and assistant alternate messages, with the user asking questions and the assistant responding. When needed, the assistant can also trigger tool messages — like calling an external API to fetch data — and the results from these tools are also captured as part of the history.

Example flow:

```r
messages = [
  SystemMessage(content="You are a helpful assistant."),               
  HumanMessage(content="What's the weather in Paris today?"),      
  AIMessage(content="Let me check that for you..."),           
  ToolMessage(content="Returns JSON weather data: { 'location': 'Paris', 'temperature': '22°C', 'condition': 'sunny' }", tool_call_id="weather-api-call"),
  AIMessage(content="It's 22°C and sunny in Paris."),     
]
```

*In this flow, each step builds on the previous one. Without chat history, the assistant wouldn't remember the user's question or its own previous actions, making it impossible to respond meaningfully.*

### What happens when chat history is missing?
Let's test it using LangChain:

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")
```

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")

llm.invoke([HumanMessage(content="Hi! At Dandilyonn, we strongly believe in the statement Earth is Worth it: that the Earth is worth saving, and we will stop at nothing to ensure that our community is a clean energy, green machine.")])
```

```r
# Output
AIMessage(content="That's a fantastic and inspiring mission! Emphasizing the importance of sustainability and a green future not only benefits the environment but also fosters a sense of community and responsibility. If you have specific initiatives, projects, or ideas you'd like to share or discuss regarding how to further promote clean energy and environmental stewardship in your community, I'd love to hear more about them! Together, we can explore ways to make your mission even more impactful.")
```

Now, if you try to ask a follow-up question without providing chat history:

```python
model.invoke([HumanMessage(content="Based on what I said earlier, what is Dandilyonn committed to?")])
```

```r
# Output
AIMessage(content='I\'m sorry, but I don\'t have access to previous conversations or any specific details about "Dandilyonn." Could you please provide more context or clarify what Dandilyonn is associated with? That way, I can assist you better!')
```

This shows that without chat history, the model treats the follow-up question as a fresh, isolated input — it doesn't know what you said earlier.

### Managing Chat History

Because the chat history carries all messages across the conversation, including tool interactions, it can quickly grow in size. However, language models have a **context window** — a maximum number of tokens they can process at once. To handle this, we need to carefully manage the chat history, trimming older or less relevant parts when necessary. Importantly, this trimming must preserve the structural integrity of the conversation: for example, the first message (often a **system** or **user** message) should remain, recent important exchanges should be kept, and **tool messages** must always directly follow the assistant's tool calls to make sense. The last message in the history should come from the **user** or **tool**, ensuring the model has the correct context for generating the next response.

> Why it matters:

- **Context Preservation**: Chat history ensures the AI remembers prior interactions, allowing it to handle follow-up questions smoothly.
- **Tool Integration**: Properly structured history enables the assistant to use external tools dynamically and weave the results into the conversation.
- **Efficiency**: Effective management of chat history avoids errors that can arise from exceeding the context window or from breaking the logical order of the conversation.

> **Agentic Workflows**
>
> *Although we haven't fully explored what an “agent” is yet, here's a little preview: when the assistant begins to use external tools — like calling a weather service to fetch live data — it starts behaving in a more agentic way. This means it's not just passively answering user questions but actively planning and executing steps to complete tasks.
In these cases, the assistant identifies that it needs extra information, makes a tool call to perform the necessary action, receives the tool message with the result, and then processes that result to give a meaningful response back to the user. All of this interaction is carefully recorded in the chat history, ensuring that the conversation remains coherent and context-aware.
> We'll dive deeper into the concept of agents later, but for now, it's useful to recognize how these more advanced workflows begin to appear within the chat history itself.*

---

# **5. Trimmer**

In conversational AI applications, such as chatbots, managing the length of the conversation history is crucial due to the limited context window of language models. LangChain addresses this challenge with the `trim_messages` utility, which helps reduce the size of a chat history to fit within a specified token count or message count, ensuring the model receives relevant and coherent context.

For instance, you can configure `trim_messages` to retain only the most recent messages, ensuring that the chat history starts with a `HumanMessage` or a `SystemMessage` followed by a `HumanMessage`, and ends with a `HumanMessage` or a `ToolMessage`. This setup maintains the logical structure expected by most chat models.

Here's a simple example:

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    trim_messages,
)
from langchain_openai import ChatOpenAI

# Initial full message history
messages = [
    SystemMessage(content="You are a helpful assistant specialized in restaurant recommendations."),               
    HumanMessage(content="Hi, can you recommend a good Italian restaurant nearby?"),      
    AIMessage(content="Sure! Are you looking for something casual or fine dining?"),           
    HumanMessage(content="Something casual, please."),                
    AIMessage(content="Got it. Would you like me to check the top-rated option on Google Maps?"),        
    HumanMessage(content="Yes, please do."),       
    AIMessage(content="Checking now..."),                           
    ToolMessage(content="Top casual Italian restaurant nearby: Luigi's Trattoria", tool_call_id="restaurant-finder"),  
    AIMessage(content="I've found a great option for you: Luigi's Trattoria. Would you like me to book a table or show the menu?"),     
]

# Trim to fit max 100 tokens
trimmed = trim_messages(
    messages,
    max_tokens=100,
    strategy="last",
    token_counter=ChatOpenAI(model="gpt-4o"),
    start_on="human",
    end_on=("human", "tool"),
    include_system=True,
)
```

Trimmed messages:

```r
trimmed = [
  SystemMessage(content='You are a helpful assistant specialized in restaurant recommendations.', additional_kwargs={}, response_metadata={}),
  HumanMessage(content='Something casual, please.', additional_kwargs={}, response_metadata={}),
  AIMessage(content='Got it. Would you like me to check the top-rated option on Google Maps?', additional_kwargs={}, response_metadata={}),
  HumanMessage(content='Yes, please do.', additional_kwargs={}, response_metadata={}),
  AIMessage(content='Checking now...', additional_kwargs={}, response_metadata={}),
  ToolMessage(content='Top casual Italian restaurant nearby: Luigi’s Trattoria', tool_call_id='restaurant-finder')
]
```

*Notice how only the most recent and essential messages are kept — earlier parts of the conversation (like the initial interaction) have been trimmed away to fit within the token limit, while the system still preserves the flow and context needed for the assistant to continue the conversation smoothly.*

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/how_to/trim_messages/" target="_blank">
      <img src="{{ '/assets/images/langchain_trim.png' | relative_url }}" alt="Langchain Trim"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        To delve deeper into how to effectively implement and configure trim_messages in your applications — ensuring you manage conversation history efficiently and maintain smooth, coherent interactions — I highly recommend following this official LangChain tutorial.
      </em>
    </p>
  </div>

</div>

---

# **6. Chatbot**

Now that we've covered essential concepts like context windows, chat history, and trimming strategies, we're ready to introduce one of the most exciting applications in LangChain:

A chatbot is an interactive system designed to hold meaningful conversations with users. In the context of LangChain, a chatbot goes far beyond a simple back-and-forth script — it is a dynamic, context-aware application that leverages language models, memory, chains, and sometimes tools to manage multi-turn interactions in a way that feels coherent, responsive, and genuinely helpful.

*However, for now, we will not be using tools yet in our chatbot examples. While tools (such as calling external APIs, databases, or calculators) are an exciting and powerful part of LangChain, they belong to a more advanced topic that we'll cover later when we dive deeper into the concept of agents.*

Let's connect this to what you've already learned:

* You now understand that a **context window** limits how much information the model can process at once.
* You know that **chat history** tracks the sequence of user and assistant messages to preserve context across turns.
* You've learned how **trimmers** help manage this history to fit within the model's input limits.

All these components come together inside a chatbot: the chatbot maintains a structured, meaningful conversation by keeping track of roles, trimming old or irrelevant history, and making sure each new response is informed by the right context.

To deepen your understanding of building chatbots with memory in LangChain, follow the official tutorial: *Build a Chatbot*. This guide introduces you to constructing a chatbot using LangChain and LangGraph, focusing on managing conversation history effectively.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/tutorials/chatbot/" target="_blank">
      <img src="{{ '/assets/images/langchain_chatbot.png' | relative_url }}" alt="Langchain Chatbot"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        While this tutorial provides a solid foundation for building chatbots with memory, it's important to note that more advanced features of LangGraph will be covered in later modules. For now, this guide offers a practical starting point to implement memory in your chatbot applications.
        By completing this tutorial, you'll gain hands-on experience with LangChain and LangGraph, preparing you for more sophisticated implementations in future lessons.
      </em>
    </p>
  </div>

</div>

---

# **7. Memory**

Although we were already introduced to the concept of memory in the previous section, the guide below takes a different approach. It offers a walkthrough similar to the previous session, but with a stronger focus on memory management itself rather than the overall chatbot architecture. Here, you'll explore memory-specific strategies more deeply, such as storing past messages and using techniques like summary memory, where an LLM call condenses long conversations to maintain coherence and efficiency in extended interactions.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/how_to/chatbots_memory/" target="_blank">
      <img src="{{ '/assets/images/lc_memory_chatbot.png' | relative_url }}" alt="Langchain Memory Chatbot"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        This guide explains how chatbots can use previous conversational turns as context to make interactions feel coherent and stateful. It covers several strategies, from simply including prior messages in the prompt, to trimming older messages to keep things efficient, and even generating conversation summaries for longer sessions. These techniques are key for making sure your chatbot doesn't just handle isolated inputs but can manage meaningful multi-turn interactions.
      </em>
    </p>
  </div>

</div>

---

# **Mini Project: Intelligent Travel Planning Chatbot** NOT FINISHED!

[resume here]

## Objective

Your task is to build a multi-turn chatbot using **LangChain + LangGraph** that helps users plan a personalized trip. This chatbot will collect relevant travel information over a natural conversation and generate a **structured JSON output** that could be used in a frontend interface.

You must use LangGraph to orchestrate the message flow and **maintain conversational memory** between turns using `MemorySaver`.

## Technical Requirements

Your solution must:

- Use **LangGraph** to define and compile the chatbot workflow
- Use **`MemorySaver`** to store memory across conversation turns
- Accept **multi-turn user inputs**, building up a travel plan step-by-step
- Use **streaming** .stream() to return the assistant's responses incrementally

## LangGraph Components

* Use `MessagesState` from LangGraph as your state schema
* Use `StateGraph` to define the workflow and add a `"model"` node
* Create a `call_model` function that adds a system prompt and invokes the LLM
* Use `MemorySaver()` as the checkpointer when compiling the graph
* Use `ChatOpenAI` as the model with `streaming=True`

## Expected Conversation Flow

1. **User starts a conversation**, e.g.:

“Hi! I want help planning my vacation.”

2. **Chatbot gathers inputs**:

* Destination
* Date
* Budget
* Preferences (e.g. museums, restaurants)

3. **Chatbot keeps the full message history** using LangGraph’s `MessagesState` and `MemorySaver`.

4. **At the end**, it generates a **structured travel plan in JSON format** with the following fields:

## Final Output Schema (Pydantic)

```python
class TravelPlan(BaseModel):
    destination: str
    date: str
    budget: str
    preferences: List[str]
    trip_summary: str
    suggestions: List[str]
```

Example JSON output:

```json
{
  "destination": "Paris",
  "date": "July 2025",
  "budget": "R$ 5.000",
  "preferences": ["museums", "restaurants"],
  "trip_summary": "Day 1: Louvre. Day 2: Eiffel Tower. Day 3: Latin Quarter.",
  "suggestions": [
    "Buy museum passes in advance",
    "Use the metro system",
    "Try prix-fixe lunches"
  ]
}
```