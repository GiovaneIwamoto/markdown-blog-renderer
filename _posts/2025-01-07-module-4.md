---
title: "Module 4: RAG - Retrieval and Generation: Enhancing Language Models with Context-Aware Retrieval and Generation"
layout: post
--- 

In this module, we dive into the second core stage of the RAG pipeline: Retrieval and Generation. This is where things start to come together — we'll retrieve relevant chunks of information from our vector database and use them to generate accurate, context-aware responses through language models. By the end of this module, you'll be able to implement a fully functional RAG flow that brings your documents to life with intelligent answers.


In the previous module, we focused on the indexing process, laying the groundwork by exploring key components such as Document Loaders, Text Splitters, Embedding Models, and Vector Stores. Now, it’s time to leverage all that structure to build an end-to-end RAG solution.

Let’s quickly recap what Retrieval-Augmented Generation (RAG) is all about. RAG is a technique that enhances the capabilities of language models by integrating them with external sources of information. Rather than relying solely on their internal training data — which can become outdated or limited — RAG systems dynamically fetch relevant content from a knowledge base at query time.

When a user makes a request, the system first retrieves the most relevant documents or data segments. This retrieved context is then passed along to the language model, enriching its prompt and allowing it to generate responses that are more accurate, up-to-date, and grounded in real information. By combining retrieval with generation, RAG bridges the gap between static knowledge and dynamic understanding — making AI systems significantly more useful and reliable in real-world applications.

---

# **1. Retrieve**

<div style="display: block; text-align: center;">

  <img src="{{ '/assets/images/lc_retrieval.jpg' | relative_url }}" alt="Langchain Pipeline Retrieval" 
       style="width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto;" />

</div>

*Image source: [LangChain Documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/)*

Now that you understand the big picture of Retrieval-Augmented Generation, it’s time to dive deeper into one of its core components: Retrieval.

Large Language Models (LLMs) are incredibly powerful, but they have one major limitation — their knowledge is frozen at the time of training. This makes it difficult for them to answer questions about recent events or provide accurate details in niche domains. While fine-tuning or continued pretraining are possible ways to inject new knowledge, these approaches are often costly, slow, and still not ideal for retrieving specific, up-to-date facts.

> This is where retrieval systems come in.
>
> By connecting your LLM to a retrieval system, you enable it to access relevant external information at the time of the query. This makes your AI not only more context-aware, but also driven by the information you feed into it.

Here’s a simple example of how the retrieval step works in a RAG pipeline.
We start with a user question, use a retriever to find relevant context from a knowledge base, and then pass that information into a language model. The model uses this retrieved context to generate a grounded and relevant answer.

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Define a system prompt that tells the model how to use the retrieved context
system_prompt = """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Context: {context}:"""
    
# Define a question
question = """What event did Dandilyonn organize to support the Climate Reality Project?""""

# Retrieve relevant documents
docs = retriever.invoke(question)

# Combine the documents into a single string
docs_text = "".join(d.page_content for d in docs)

# Populate the system prompt with the retrieved context
system_prompt_fmt = system_prompt.format(context=docs_text)

# Create a model
model = ChatOpenAI(model="gpt-4o", temperature=0) 

# Generate a response
questions = model.invoke([SystemMessage(content=system_prompt_fmt),
                          HumanMessage(content=question)])
```

Here’s what the full RAG flow looks like in practice — from question to context retrieval to a generated answer, all working together to deliver informed responses.

> Question:
> What event did Dandilyonn organize to support the Climate Reality Project?
>
>
> Retrieved Context:
> "To fundraise for Al Gore's Climate Reality Project, Dandilyonn hosted our first ever walkathon. In addition to raising funds, it was a statement that the Dandilyonn Community will never stand down in this fight against climate change."
>
> Generated Answer:
> Dandilyonn organized its first-ever walkathon to raise funds for Al Gore's Climate Reality Project and to show its commitment to fighting climate change.

*Reference: [LangChain Concepts](https://python.langchain.com/docs/concepts/rag/)*

---

# **2. Generate**

---

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval?utm_source=profile&utm_medium=reader2" target="_blank">
      <img src="{{ '/assets/images/rag_advanced.png' | relative_url }}" alt="Rag Article"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        For those who want to go beyond the basics and gain a deeper understanding of how Retrieval-Augmented Generation (RAG) truly works, I highly recommend reading A Practitioner’s Guide to RAG by Cameron R. Wolfe, Ph.D. This article goes beyond simple tutorials and explores RAG from a practical, yet technically insightful perspective — covering foundational concepts, pipeline structure, chunking strategies, hybrid search, prompt design, evaluation methods (like RAGAS), and even performance tuning. It’s an excellent resource to understand not just how to implement RAG, but also why certain design decisions matter when building scalable, trustworthy AI systems.
      </em>
    </p>
  </div>

</div>