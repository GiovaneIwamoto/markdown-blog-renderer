---
title: "Module 3: RAG - Indexing: Document Loaders, Text Splitters, Embedding Models, Vector Stores"
layout: post
--- 

In this module, we'll focus specifically on the indexing phase of Retrieval-Augmented Generation (RAG). Indexing is the foundation of any RAG pipeline, involving the transformation of raw text into vector embeddings and storing them in a way that enables efficient retrieval later. While this step doesn't generate responses yet, it is essential for preparing the knowledge base that language models will use.


We'll introduce the overall concept of RAG and briefly explain how it connects the retrieve and generate steps to large language models (LLMs). However, the full implementation and development of retrieval strategies and answer generation will be covered in the next module, where we'll go deeper into how those components interact with the index we build here.

<div style="display: block; text-align: center;">

  <img src="{{ '/assets/images/langchain_indexing.png' | relative_url }}" alt="Langchain Indexing" 
       style="width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto 10px auto;" />

  <img src="{{ '/assets/images/langchain_retrieval.png' | relative_url }}" alt="Langchain Retrieval" 
       style="width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto;" />

</div>

*Image source: [LangChain Documentation](https://python.langchain.com/docs/tutorials/rag/)*

To build a RAG system, we'll cover the full pipeline: loading data from various sources, splitting long texts into chunks, embedding the content into vectors, storing it in vector databases, retrieving the most relevant pieces during a query, and finally using that retrieved information to generate informed answers.

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/T-D1OfcDW1M" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/zYGDpG-pTho" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

</div>

*Before we begin exploring each topic individually, here's a brief overview of the core components involved in the RAG workflow. Each step plays a crucial role in enabling language models to retrieve and use external information effectively:*

* **Document Loaders**: Responsible for bringing external data into the system. They support a wide variety of formats such as PDFs, web pages, databases, and cloud files, acting as the entry point to your knowledge base.

* **Text Splitters**: Once documents are loaded, they are broken down into smaller, more manageable chunks. This helps ensure that information remains coherent and accessible during processing and retrieval.

* **Text Embedding Models**: These models convert chunks of text into numerical vectors that capture the meaning and context of the content. This transformation enables semantic search, where similar concepts are positioned closely in vector space.

* **Vector Stores**: Specialized databases that store and organize text embeddings. They allow for fast and meaningful retrieval of content and are compatible with in-memory setups or scalable cloud-based solutions.

> *Note: In this module, we'll focus on these indexing steps. The next module will explore in depth how retrieval and generation work together to produce accurate, context-aware responses.*

* **Retrievers**: Tools that search the vector store to find the most relevant pieces of information based on a user query. Different types of retrievers exist, from simple semantic matchers to more advanced ones like self-query retrievers or ensemble approaches that combine multiple strategies.

* **Generation**: Finally, the language model takes the retrieved context and uses it to generate responses that are more accurate, grounded, and relevant to the query.

*The video below offers a high-level overview of the key topics we'll be working on in this module. It serves as a helpful preview of what's to come, and each component will be explored individually and in more depth as we move forward.*

{% include embed.html url="https://www.youtube.com/embed/tcqEUSNCn8I" %}

---

# **1. Document Loaders**

Document Loaders are responsible for bringing external content—like CSVs, PDFs, web pages, and databases—into your application in a format that LangChain can work with. Regardless of the source, all loaders return data in a unified structure called a `Document`, which includes both the content and relevant metadata.

Each type of loader is designed to handle a specific format or data source, and while their configuration parameters vary depending on the integration, they all share a common interface: you initialize the loader and call its `.load()` method to extract the data.

```python
# Load CSV file
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader( 
    ...  # Your custom parameters here
)
documents = loader.load()
```

```python
# Load PDF file
from langchain_community.document_loaders import PyPDFLoader

file_path = "./example_data/layout-parser-paper.pdf"
loader = PyPDFLoader(file_path)

documents = loader.load()
```

<div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/integrations/document_loaders/" target="_blank">
      <img src="{{ '/assets/images/lc_document_loaders.png' | relative_url }}" alt="Langchain Document Loaders"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/how_to/#document-loaders" target="_blank">
      <img src="{{ '/assets/images/lc_how_to.png' | relative_url }}" alt="Langchain How To Document Loaders"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>

  <div style="flex: 1;">
    <p>
      <em>
        To explore how document loaders work and understand the different types available, the links provided offer both a practical guide to using them effectively and a broader overview of their role in connecting AI workflows to real-world data.
      </em>
    </p>
  </div>

</div>

---

# **2. Text Splitters**

Text splitters are utilities that break a large document into smaller chunks of text. In a Retrieval-Augmented Generation pipeline, this step is crucial: it transforms long knowledge sources into bite-sized pieces that can be embedded and retrieved efficiently. Large Language Models have a limited context window, meaning they cannot consume extremely long texts all at once.

By splitting a document into chunks that fit the context window, we ensure the LLM can later ingest relevant pieces. This process, commonly referred to as **chunking**, is fundamental to RAG. After splitting, each chunk is vectorized and stored so that, when a query is made, only the most relevant chunks are retrieved for the model to generate a response.

In short, text splitters bridge the gap between dense source data and LLM input limits, enabling applications like question answering over documents.

{% include embed.html url="https://www.youtube.com/embed/Io43kf0hYn4" %}

Another reason text splitters are important is semantic coherence. Ideally, we want each chunk to contain a self-contained, semantically meaningful piece of information. 

If we naively split text at fixed lengths, we might cut off sentences or separate closely related context. Good text splitting keeps related pieces of text together – for example, a paragraph or a section of a document should remain in one chunk if possible. Maintaining this coherence improves the quality of downstream embeddings and results, since each chunk will represent a complete thought or topic.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://chunkviz.up.railway.app" target="_blank">
      <img src="{{ '/assets/images/chunk_viz.png' | relative_url }}" alt="Chunkviz"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        To better visualize how different chunking strategies work and understand their impact, check out this interactive tool: ChunkViz v0.1. It's a helpful resource for exploring various text splitting approaches in the context of language models.
      </em>
    </p>
  </div>

</div>

## How Text Splitters Work in LangChain

LangChain provides built-in text splitter classes that implement common chunking strategies. Under the hood, the chunking process can be summarized in three steps:

**Initial Split** – Break the text into small units (e.g. sentences or paragraphs) using a separator or delimiter. This yields many small pieces that are semantically meaningful (for example, splitting on sentence boundaries).

**Chunk Assembly** – Gradually combine those small pieces into a larger chunk until a certain size limit is reached (the chunk size). This forms a chunk that is under or around the desired maximum length.

**Overlap and Next Chunk** – Once a chunk reaches the size limit, finalize it and start a new chunk. Optionally include some overlap with the previous chunk (repeating the last few tokens/words) to preserve context continuity between chunks. This overlap helps to avoid cutting important information that straddles a boundary.

This chunking approach aims to produce text segments that are constrained in size while still preserving the natural structure of the original content. Instead of arbitrarily cutting text at fixed lengths—which can disrupt meaning or break sentences—this method prioritizes splitting at logical boundaries such as paragraphs or sentences. 

There are two important aspects you can configure to fine-tune how chunking behaves. First, how the text is split: this could be based on characters, words, sentences, or specific delimiters like newlines or paragraph breaks. Choosing the right splitting method helps maintain the coherence of each chunk. Second, how the chunk size is measured: chunk limits can be defined by the number of characters, the number of tokens (which is especially relevant for language models), or even by higher-level semantic units. 

By thoughtfully adjusting these two factors, you can ensure that each chunk remains meaningful, balanced in size, and well-suited for downstream tasks like embedding or retrieval.

<div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
  <div style="flex: 0 0 20%;">
    <a href="https://python.langchain.com/docs/how_to/#text-splitters" target="_blank">
      <img src="{{ '/assets/images/lc_how_to.png' | relative_url }}" alt="Langchain How To Text Splitters"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 0 0 20%;">
    <a href="https://python.langchain.com/docs/concepts/text_splitters/" target="_blank">
      <img src="{{ '/assets/images/lc_text_splitters.png' | relative_url }}" alt="Langchain Text Splitters"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>

  <div style="flex: 1;">
    <p>
      <em>
        For more information on the different types of text splitters and how to choose the right one for your use case, check out the official documentation linked in the images.
      </em>
    </p>
  </div>

</div>

---

# **3. Embedding Models**

{% include embed.html url="https://www.youtube.com/embed/vlcQV4j2kTo" %}

**Text embeddings** are numerical representations that capture the meaning and context of a piece of text within a high-dimensional vector space. Unlike traditional approaches that rely on matching exact words, embeddings allow machines to evaluate the *semantic similarity* between texts — understanding what the text *means*, not just what it *says*.

This capability begins with **text vectorization**, the process of converting text into a numerical format that models can work with. While early techniques like *one-hot encoding* or *bag-of-words* count word occurrences, they fall short in capturing relationships between words or the context in which they appear. Modern embedding models overcome this limitation by learning to position semantically similar texts closer together in a shared vector space. This allows for more intelligent and flexible comparisons across different pieces of text.

Embeddings have become essential in many NLP applications, including **semantic search**, recommendation systems, clustering, and classification. By using similarity metrics such as **cosine similarity**, systems can quantitatively assess how closely two texts are related in meaning — a core feature for making language-based applications more accurate and adaptive.

OpenAI offers state-of-the-art embedding models that generate high-dimensional, fixed-length vectors capable of representing complex textual meaning. These models can be easily integrated into applications using LangChain, which provides convenient tools for embedding generation, storage in vector databases, and semantic retrieval.

This leads us to a critical application of embeddings in Retrieval-Augmented Generation. In a RAG pipeline, when a user submits a query, it is first embedded into a vector. This query vector is then compared against a pre-indexed set of document embeddings stored in a vector database. Instead of relying on exact keyword matches, the system retrieves documents whose semantic meaning closely aligns with the user's input — even if the words used are different.

These retrieved documents are then provided as context to the language model, enabling it to **generate** more accurate, relevant, and grounded responses. The quality of the final answer depends heavily on the **retrieval** step, and by extension, on the quality of the embeddings used.

In summary, embeddings are not just useful for comparing texts — they are foundational in enabling retrieval-based architectures like RAG to operate effectively. By serving as the bridge between a user's intent and the relevant knowledge available, embeddings ensure that generative AI systems respond with precision, relevance, and contextual understanding.

*Here is an example using LangChain together with an embedding model from OpenAI to generate vector representations of text.*

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")

embeddings = embeddings_model.embed_documents(
    ["At Dandilyonn, we strongly believe in the statement Earth is Worth it: that the Earth is worth saving, and we will stop at nothing to ensure that our community is a clean energy, green machine. Our mission is to educate youth about climate change, promote green habits, and raise money for community projects beneficial to the environment.",]
)
len(embeddings), len(embeddings[0])
```

```python
# Output
(1, 3072)
```

```python
embedded_query = embeddings_model.embed_query("What is SEEDS and what is its main purpose?")
embedded_query[:6]
```

```python
# Output
[-0.02120787650346756,
 0.0007380267488770187,
 -0.02360207587480545,
 -0.036326803267002106,
 0.00472189299762249,
 -0.032632049173116684]
```

*For more details about OpenAI's embedding models, including available options and technical specifications, please visit the official documentation at the link below:*

*[Check this link for more information on OpenAI embedding models](https://platform.openai.com/docs/guides/embeddings#embedding-models)*

{% include embed.html url="https://www.youtube.com/embed/wgfSDrqYMJ4" %}

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/concepts/embedding_models/" target="_blank">
      <img src="{{ '/assets/images/lc_embedding_models.png' | relative_url }}" alt="Langchain Embedding Models"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation. This is the power of embedding models, which lie at the heart of many retrieval systems. Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.
      </em>
    </p>
  </div>

</div>

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://www.mongodb.com/resources/basics/vector-embeddings" target="_blank">
      <img src="{{ '/assets/images/mongo_db_vector_embeddings.png' | relative_url }}" alt="MongoDB Vector Embeddings"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        For more information on vector embeddings, refer to this resource.
        Vector embeddings are mathematical representations that translate text and other types of data into numerical form. They serve as a bridge between human language and machine learning systems by capturing the meaning of content in a format that models can process and analyze.
      </em>
    </p>
  </div>

</div>

---

# **4. Vector Stores**

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/ebMkbWzFCnA" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

  <iframe style="width: calc(50% - 10px); aspect-ratio: 16 / 9; border: none;" 
          src="https://www.youtube.com/embed/5o5Ds8KLqVw" 
          title="YouTube video player" 
          allowfullscreen>
  </iframe>

</div>

A **vector database** is a specialized database designed for storing and searching these high-dimensional embedding vectors. It indexes and stores vector embeddings and allows fast retrieval based on **similarity search**, rather than exact matching. In fact, the terms vector database, vector store, or vector search engine often refer to the same idea – a system that can store vectors (fixed-length lists of numbers) and query them by similarity. Unlike traditional databases that excel at exact matches and structured queries (like finding a record by an ID or filtering by a field value), a vector database is built to answer questions like “*Which stored vectors are most similar to my query vector?*”. It typically implements nearest neighbor search algorithms under the hood. This means you can give it a new vector (for example, an embedding of a user's question) and it will quickly return the stored items whose embeddings are closest to that vector – i.e., the most semantically relevant items.

Importantly, a vector database isn't just a raw index; it usually provides all the standard database features needed to manage the data. A good vector database supports CRUD operations (Create, Read, Update, Delete), can store metadata alongside each vector (like an ID or tags for the original text), allows filtering results by metadata, and can scale to handle large amounts of data efficiently. In essence, it combines the ability to do similarity searches on vectors with the reliability and management features of a database.

## Limitations of Traditional Databases for Vector Search

Traditional relational databases or key-value stores are not optimized for the kind of similarity matching we need with embeddings. You could store vectors in them, but searching for "the closest vector" would be extremely slow without specialized indexes. As embeddings have hundreds or thousands of dimensions, doing a naive search is computationally heavy. Traditional databases are built for exact matches or simple numeric comparisons, not for high-dimensional geometry. This is why we need a specialized system: vector databases are designed to handle the complexity and scale of embedding data that would overwhelm a normal database. They use advanced indexing techniques (like approximate nearest neighbor algorithms) to make similarity search fast, even over millions of vectors.

## How Vector Databases Power Semantic Retrieval

Now that we understand embeddings and how they represent the meaning of text, it's important to see how vector databases enable efficient semantic search—especially in applications where an AI system needs to respond based on external content.

Instead of relying solely on a model's pre-trained knowledge, we can give it access to relevant documents through a process that involves retrieving information and using it to generate more accurate responses. The vector database plays a central role in making this possible by storing embeddings and allowing fast similarity-based retrieval.

Here's how the process works:

* **Indexing (Prepare the Knowledge Base)**: All documents are split into smaller chunks, embedded using a model, and stored in a vector database. Each vector is linked to its original text so it can be retrieved later when relevant.

> *Note: This indexing step is everything we've done so far: transforming raw text into embeddings and storing them in a structure optimized for semantic search.*

* **Query Encoding (Understand the Input)**: When the user provides an input—such as a prompt, instruction, or search query—the system converts it into an embedding using the same model used during indexing.

* **Similarity Search (Retrieve Relevant Information)**: The vector database compares the query vector with the stored ones and returns the closest matches—those that are most similar in meaning.

* **Answer Generation (Use Retrieved Context)**: The retrieved text chunks are sent to the language model alongside the query, providing the context it needs to generate a well-informed answer.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/concepts/vectorstores/" target="_blank">
      <img src="{{ '/assets/images/lc_vector_stores.png' | relative_url }}" alt="Langchain Vector Stores"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        To learn more about how LangChain utilizes vector stores for semantic search and efficient information retrieval, follow this documentation. This page provides an overview of vector stores, their role in indexing and retrieving data based on vector representations, and how they integrate with LangChain to enhance applications like chatbots and question-answering systems.
      </em>
    </p>
  </div>

</div>

## Pinecone: A Scalable Vector Database

*At this point, you will need to create an account on Pinecone to continue. This will give you access to your personal API key, which is required to interact with the Pinecone vector database service. With your API key, you'll be able to set up indexes, perform vector searches, and integrate Pinecone into your Retrieval-Augmented Generation pipeline. Make sure to store your key securely and follow best practices when using it in your applications.*

{% include embed.html url="https://www.youtube.com/embed/videoseries?si=7yE6tl2okuCOgXEx&amp;list=PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-" %}


<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://www.pinecone.io" target="_blank">
      <img src="{{ '/assets/images/pinecone_site.png' | relative_url }}" alt="Pinecone Website"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Pinecone is a fully managed vector database designed for high-performance similarity search over large-scale embedding data. It allows developers to store, index, and query vector representations of text with low latency and high accuracy. Pinecone is especially useful in Retrieval-Augmented Generation applications, where retrieving semantically relevant documents is critical. Its ease of integration with tools like LangChain and support for real-time updates make it a powerful choice for building scalable, intelligent AI systems.
      </em>
    </p>
  </div>

</div>

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" target="_blank">
      <img src="{{ '/assets/images/pinecone_rag.png' | relative_url }}" alt="Pinecone RAG"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Supplementary Reading: What is Retrieval-Augmented Generation (RAG)? (Pinecone's Perspective)
        Retrieval augmented generation is an architecture that provides the most relevant and contextually-important proprietary, private or dynamic data to your Generative AI application's large language model (LLM) when it is performing tasks to enhance its accuracy and performance.
      </em>
    </p>
  </div>

</div>

## Pinecone within Langchain

This example demonstrates how to use OpenAI's embedding model and integrate it with Pinecone for vector storage using the LangChain framework. First, the OpenAI API key is securely loaded and used to initialize the OpenAIEmbeddings class with the `text-embedding-3-large` model, which is designed to generate high-quality vector representations of text. Then, the `PineconeVectorStore` is instantiated by providing the embeddings and a reference to a Pinecone index, which is created using your Pinecone API key and index name. This setup allows you to store and retrieve vectorized documents efficiently.

```python
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

```python
pip install -qU langchain-pinecone  
```

```python
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

pc = Pinecone(api_key=...)
index = pc.Index(index_name)

vector_store = PineconeVectorStore(embedding=embeddings, index=index)
```

*For more examples and different ways to initialize vector stores, follow this link: [LangChain VectorStores Documentation](https://python.langchain.com/docs/integrations/vectorstores/)*

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/integrations/vectorstores/pinecone/#setup" target="_blank">
      <img src="{{ '/assets/images/lc_pinecone.png' | relative_url }}" alt="Pinecone within Langchain"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        This tutorial presents how to initialize and manage a Pinecone vector store within the LangChain framework. It guides you through the setup process, including the installation of essential packages such as langchain, langchain-pinecone, and langchain-openai. Additionally, it provides a brief introduction to performing a simple similarity search using the initialized vector store. By following this tutorial, you will learn how to prepare and operate a vector database to support AI applications using LangChain and Pinecone.
      </em>
    </p>
  </div>

</div>

## Dimensions in Embedding Models and Vector Indexes

If you've worked with OpenAI's embedding models—like `text-embedding-3-small` you've probably noticed that each one produces vectors of a specific size. Likewise, when setting up a vector index in systems like Pinecone, you're required to define the number of dimensions the index will accept.

This number isn't arbitrary—it must match the dimensionality of the vectors generated by your embedding model. But what exactly does that mean?

When you embed a piece of text using a model, it returns a vector—a long list of numbers that represents the meaning of the input in a machine-readable form. Each number in this vector is called a dimension, and together, they form a position in a high-dimensional space that captures the semantic content of the text. For example, a model that outputs 1536 values produces a 1536-dimensional embedding.

Understanding what these dimensions represent—and why they matter—is the foundation for building effective, compatible vector search systems.

## Visualizing Dimensions in Vector Databases

The idea of *dimensions* in an embedding can seem abstract at first—but there's a simple way to build intuition around what it means. Imagine each embedding as a point in a geometric space. To simplify:

> In a **2D space**, a point like `(x,y)` might represent a sentence based on just two features—such as sentiment and formality. Texts with similar values on both axes would appear close together.

> In a **3D space**, adding a third axis `(x,y,z)` might allow you to capture something like topic or tone. This makes it easier to separate texts that are similar in some ways but different in others.

Now, extend this idea: instead of just two or three axes, imagine a space with many axes—each capturing a subtle property of the input text. You may not be able to name each one, but together they define a meaningful position for that text in a high-dimensional space.
Vector databases rely on this structure. They store these points and use mathematical distance to compare them. The closer two vectors are in this space, the more similar their underlying meanings.
While each dimension may be abstract, together they define a space that effectively encodes the meaning and context of the input.

> **Embedding Dimensions Aren't Human Labels**
>
> After understanding the spatial analogy, it's important to clarify a common misconception: the dimensions of an embedding don't map directly to specific human concepts.
>
> You won't find a single dimension that means “sarcasm,” “positivity,” or “formality.” Embeddings are not built with labeled axes. Instead, their strength comes from how models learn statistical patterns across large amounts of data. Each dimension contributes a small part to the overall meaning, and it's the combination of all dimensions that allows the vector to represent complex ideas.
>
> For example, the semantic difference between “king” and “queen” isn't captured by flipping a single “gender” switch. Rather, it's reflected in how multiple dimensions shift together to adjust for roles, context, and relationships the model has seen during training.
>
> This ability to capture meaning through patterns—rather than fixed categories—is what makes embeddings so powerful. They allow vector databases to compare texts not by exact wording, but by deeper semantic similarity. That's how we can retrieve relevant results even when queries and documents use different vocabulary or phrasing.

## How Dimensionality Affects Representation Quality

One reason embeddings are so effective in vector databases is their ability to encode complex information across many dimensions. More dimensions give the model more *space* to separate similar but distinct concepts, improving the granularity of similarity search.

However, there's a limit. More dimensions also mean higher memory usage, longer search times, and potentially reduced generalization if the space becomes too sparse. In practice, increasing dimensionality leads to diminishing returns beyond a certain point.
 
Some embedding models allow you to reduce vector size—trading a bit of accuracy for faster, more efficient operations. Choosing the right balance depends on your use case and performance goals.

## Ensuring Compatibility: Matching Models and Indexes

When creating an index in Pinecone—or any vector database—you're required to define how many dimensions each vector should have. This setting must exactly match the dimensionality of the vectors produced by your embedding model. Otherwise, the system will reject the input and throw an error.

For example, OpenAI's embedding models generate vectors of fixed sizes:

- `text-embedding-3-small` → produces vectors with 1536 dimensions
- `text-embedding-3-large` → produces vectors with 3072 dimensions

If you try to insert a vector from a larger model into an index configured for fewer dimensions, the operation will fail. That's why dimensional alignment between your embedding model and vector index is not optional—it's essential. These configuration choices are tightly coupled and directly affect:

- Reliable vector ingestion and indexing
- Accurate similarity search results
- Stable and predictable pipeline

*Whenever you change embedding models or modify your index settings, always double-check dimensional compatibility. It's a small step, but critical for system integrity. Watch the video below for a quick recap of all the concepts we've covered so far.*

{% include embed.html url="https://www.youtube.com/embed/dN0lsF2cvm4" %}

---

# Mini Project: Semantic Indexing of Scientific Articles

In this project, you will implement the indexing phase of a RAG pipeline, focusing on preparing scientific documents for semantic search. Your task is to build a searchable knowledge base using LangChain and Pinecone by transforming PDF articles into embeddings and storing them in a vector database.

This project covers key concepts such as document loading, chunking, semantic embeddings, and vector indexing — essential steps for enabling AI models to access and reason over external knowledge.

You will work with real-world academic PDFs (from *arXiv* or open-access journals), and at the end of the process, you'll be able to issue a natural language query and retrieve semantically relevant excerpts from the indexed documents.

### What You Will Build

- A pipeline that loads PDFs, splits them into chunks, and generates semantic embeddings
- A Pinecone vector index populated with your embedded text chunks
- A simple search script that accepts a query and returns the *top-N* most relevant text passages, along with metadata like article title or page number

### Project Steps and Tips

- **Loading PDFs**

Start by using LangChain’s `PyPDFLoader` to load your scientific articles. This loader parses each page of the PDF and returns a list of Document objects. Each object contains both text `page_content` and metadata such as the page number or file path.

```python
loader = PyPDFLoader("path/to/article.pdf")
pages = loader.load()
```

You can loop through multiple PDFs later by extending this logic with `os.listdir()` or using a batch loader function.

- **Splitting Text into Chunks**

Because language models and embedding models work best with limited input size, you’ll need to split the text into manageable segments. Use `RecursiveCharacterTextSplitter`, which attempts to preserve semantic boundaries like paragraphs or sentences.

Set a `chunk_size` and an `chunck_overlap` to help preserve context between chunks. This overlap is especially helpful for avoiding cutoff in the middle of sentences or concepts.

```python
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(pages)
```

Make sure to inspect the result and validate whether each chunk makes sense by itself.

- **Enriching Metadata**

Each chunk already inherits metadata from its source page, but you can enrich this with additional fields like:

- `title`: Extracted from the first page or filename
- `source`: File path or article identifier
- `category`: The thematic classification of the content

This will help you later filter or organize results.

```python
for chunk in chunks:
  chunk.metadata["title"] = "AI in Medicine"
```

- **Generating Embeddings**

Use the `OpenAIEmbeddings` class. Remember to set your API key securely and ensure you are using a model like `text-embedding-3-small` or similar.

Avoid embedding raw documents — always embed chunks, as each one will be an individual record in the index.

- **Indexing into Pinecone**

Once embeddings are generated, connect to your Pinecone account using the pinecone Python client. Make sure your index is initialized with the correct number of dimensions (e.g., 1536 for `text-embedding-3-small`).

Use `PineconeVectorStore.from_documents()` to push your documents and embeddings into the vector database. All metadata will be automatically associated if it's present in each Document.

Check that you’ve correctly defined your Pinecone environment and API key:

```python
from pinecone import Pinecone

pc = Pinecone(api_key="...")
index = pc.Index("your-index-name")
```

- **Testing Semantic Search**

Once indexed, test the search using a natural language query. Use `similarity_search()` to retrieve the top results by semantic closeness.

```python
results = vectorstore.similarity_search("How is machine learning used in healthcare?")
```

Print both the content and metadata of the returned chunks to confirm the quality of your index.

### Optional Extensions

* Add batch upload from a folder of PDFs
* Add filters to the search based on metadata (e.g., year of publication)
* Log embeddings and metadata into a `.csv` or `.json` file for auditability
